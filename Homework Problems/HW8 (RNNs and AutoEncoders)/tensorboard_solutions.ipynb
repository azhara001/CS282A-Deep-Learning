{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import wandb\n",
    "from architectures import BasicConvNet, ResNet18, MLP\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Tensorboard\n",
    "Tensorboard is a local tool for visualizing images, metrics, histograms, and more. It is designed for tensorflow, but can be integrated with torch. Let's explore tensorboard usage with an example:\n",
    "\n",
    "```\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# To start a run, call the following\n",
    "writer = SummaryWriter(comment=f'Name_of_Run')\n",
    "\n",
    "# When you want to log a value, use the writer. When adding a scalar, the format is as follows: \n",
    "# add_scalar(tag, scalar_value, global_step=None, walltime=None, new_style=False, double_precision=False)\n",
    "writer.add_scalar('Training Loss', loss.item(), step)\n",
    "\n",
    "# Finally, when you are done logging values, close the writer\n",
    "writer.close()\n",
    "```\n",
    "There are many other functionalities and methods that you are free to explore, but will not be mentioned in this notebook.\n",
    "\n",
    "## Your Task\n",
    "We will be once again building classifiers for the CIFAR-10. There are various architectures set up for you to use in the architectures.py file. Using tensorboard, please search through 5 different hyperparameter configurations. Examples of choices include: learning rate, batch size, architecture, optimization algorithm, etc. Please submit the generated plots on your pdf and answer question A. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 2\n",
    "cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                    download=True, transform=transform)\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rates = [1e-3, 1e-4]\n",
    "batch_sizes = [128, 256]\n",
    "log_freq = 2\n",
    "eval_freq = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(learning_rate, batch_size):\n",
    "    \n",
    "    writer = SummaryWriter(comment=f'_LR_{learning_rate}_BS_{batch_size}')\n",
    "    model = MLP().to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                            shuffle=True, num_workers=2)\n",
    "    \n",
    "    testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                            shuffle=True, num_workers=2)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    step = 0\n",
    "    \n",
    "    for _ in range(int(epochs * 128/batch_size)):  # loop over the dataset multiple times\n",
    "        dataiter = iter(testloader)\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            inputs = torch.flatten(inputs, start_dim=1)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            accuracy = torch.mean((torch.argmax(outputs, dim=1) == labels).float()).item() * 100\n",
    "\n",
    "\n",
    "            if i % log_freq == 0:\n",
    "                writer.add_scalar('Training Loss', loss.item(), step)\n",
    "                writer.add_scalar('Training Accuracy', accuracy, step)\n",
    "            if i % eval_freq == 0:\n",
    "                eval_inputs, eval_labels = next(dataiter)\n",
    "                eval_inputs, eval_labels = eval_inputs.to(device), eval_labels.to(device)\n",
    "                eval_inputs = torch.flatten(eval_inputs, start_dim=1)\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    eval_outputs = model(eval_inputs)\n",
    "                eval_loss = criterion(eval_outputs, eval_labels)\n",
    "                model.train()\n",
    "                eval_accuracy = torch.mean((torch.argmax(eval_outputs, dim=1) == eval_labels).float()).item() * 100\n",
    "                writer.add_scalar('Validation Loss', eval_loss.item(), step)\n",
    "                writer.add_scalar('Validation Accuracy', eval_accuracy, step)\n",
    "            step += 1\n",
    "            print(f'\\rStep: {step}', end='')\n",
    "\n",
    "   \n",
    "    print(f'\\nRun with (lr: {learning_rate}, batch_size: {batch_size}) has finished')\n",
    "    writer.close()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 782\n",
      "Run with (lr: 0.001, batch_size: 128) has finished\n"
     ]
    }
   ],
   "source": [
    "for lr in learning_rates:\n",
    "    for batch_size in batch_sizes:\n",
    "        run(lr, batch_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "projects",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
